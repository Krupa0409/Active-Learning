# -*- coding: utf-8 -*-
"""ML Assignment Active Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CIlJ-fbeot-QC-vRTP2kzdHHCvUNUU0X
"""



# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from modAL.models import ActiveLearner
from modAL.uncertainty import uncertainty_sampling,entropy_sampling,margin_sampling
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from matplotlib import pyplot as plt
# %matplotlib inline

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=entropy_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 10% of the data points using pool based active learning
N_QUERIES=int(len(X)*10/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here entropy has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 10% additional points using entropy uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial =1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=entropy_sampling,
    # query_strategy=uncertainty_sampling,
    # query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 20% of the data points using pool based active learning
N_QUERIES=int(len(X)*20/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here entropy has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 20% additional points using entropy uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=entropy_sampling,
    # query_strategy=uncertainty_sampling,
    # query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 30% of the data points using pool based active learning
N_QUERIES=int(len(X)*30/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here entropy has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 30% additional points using entropy uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initializing our ActiveLearner object with only labelled training points
n_initial = 1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=entropy_sampling,
    # query_strategy=uncertainty_sampling,
    # query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 40% of the data points using pool based active learning
N_QUERIES=int(len(X)*40/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here entropy has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 40% additional points using entropy uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    # query_strategy=entropy_sampling,
    query_strategy=uncertainty_sampling,
    # query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 10% of the data points using pool based active learning
N_QUERIES=int(len(X)*10/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here least confidence has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 10% additional points using least confidence uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    # query_strategy=entropy_sampling,
    query_strategy=uncertainty_sampling,
    # query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 20% of the data points using pool based active learning
N_QUERIES=int(len(X)*20/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here least confidence has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 20% additional points using least confidence uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    # query_strategy=entropy_sampling,
    query_strategy=uncertainty_sampling,
    # query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 30% of the data points using pool based active learning
N_QUERIES=int(len(X)*30/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here least confidence has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 30% additional points using least confidence uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    # query_strategy=entropy_sampling,
    query_strategy=uncertainty_sampling,
    # query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 40% of the data points using pool based active learning
N_QUERIES=int(len(X)*40/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here least confidence sampling has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 40% additional points using least confidence uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 10% of the data points using pool based active learning
N_QUERIES=int(len(X)*10/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here margin sampling has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 10% additional points using margin uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    # query_strategy=entropy_sampling,
    # query_strategy=uncertainty_sampling,
    query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 20% of the data points using pool based active learning
N_QUERIES=int(len(X)*20/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here margin sampling has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 20% additional points using margin uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 1
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    # query_strategy=entropy_sampling,
    # query_strategy=uncertainty_sampling,
    query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 30% of the data points using pool based active learning
N_QUERIES=int(len(X)*30/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here margin sampling has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 30% additional points using margin uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

# Loading MNIST dataset of handwritten digits into variables X & y where- 
# X:8x8 image of handwritten digit
# Y:Label between 0-9 (Total 10 classes)
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    # query_strategy=entropy_sampling,
    # query_strategy=uncertainty_sampling,
    query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)

# Now we will use pool based active learning
# Here we seperate the points on which model was initially trained from the points which will serve as human oracle for pool based scenario
X_pool = np.delete(X,initial_idx , axis=0)
y_pool = np.delete(y, initial_idx , axis=0)
final_accuracy=0
# Labelling additional 40% of the data points using pool based active learning
N_QUERIES=int(len(X)*40/100)
# arr=np.asarray(X_pool)
# The following loop allows our model to query our unlabeled dataset for the most
# informative points according to our uncertainty sampling measure(Here margin sampling has been used).
for index in range(N_QUERIES):
  # Gets the most informative point
  query_index, query_instance = learner.query(X_pool)

  # Teach our ActiveLearner model the record it has requested.
  X_query, y_query = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=X_query, y=y_query)

  # Remove the queried instance from the unlabeled pool.
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
  # Calculate and report our model's accuracy.
  model_accuracy = learner.score(X_test, y_test)
  print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_index,n=index + 1, acc=model_accuracy))
  final_accuracy=model_accuracy
print('Accuracy of the model after labelling 40% additional points using margin uncertainity measure is:{acc:0.4f}'.format(m=query_index,n=index + 1, acc=final_accuracy))

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=entropy_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*10/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_entropy(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 10% of labelled points using entropy measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=entropy_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*20/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_entropy(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 20% of labelled points using entropy measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=entropy_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*30/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_entropy(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 30% of labelled points using entropy measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=entropy_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*40/100)
final_accuracy=0
while index<N_QUERIES:
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_entropy(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        final_accuracy=new_score
        index=index+1
print('Final accuracy of the stream based learning object after adding 40% of labelled points using entropy measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*10/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_margin(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 10% of labelled points using margin measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*20/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_margin(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 20% of labelled points using margin measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*30/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_margin(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 30% of labelled points using margin measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=margin_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*40/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_margin(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 40% of labelled points using margin measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=uncertainty_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*10/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_uncertainty(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 10% of labelled points using least confidence measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=uncertainty_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*20/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_uncertainty(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 20% of labelled points using least confidence measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=uncertainty_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*30/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_uncertainty(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 30% of labelled points using least confidence measure is:',final_accuracy)

from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Randomly selects and retains only 10% of labelled points.The removed labelled points will work as human oracle
random_idx = np.random.choice(range(len(X)), size=int(len(X)/10), replace=False)
X_train, y_train = X[random_idx], y[random_idx]

# Initializing our ActiveLearner object with only labelled training points
n_initial = 10
initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)
X_train1, y_train1 = X_train[initial_idx], y_train[initial_idx]
# The model which we will be using will be an ActiveLearner Object of modAl package.
# It takes two parameters-Estimator(model used for predicting class),query strategy
# Here the model we are using is the Random Forest Classifier which is an ensemble of decision trees.
learner = ActiveLearner(
    estimator=RandomForestClassifier(),
    query_strategy=uncertainty_sampling,
    X_training=X_train1, y_training=y_train1
)

# This is the initial accuracy of our model
unqueried_score = learner.score(X_test, y_test)

print('Initial prediction accuracy of the model was: %f' % unqueried_score)
X_stream = np.delete(X,random_idx,axis=0)
y_stream=np.delete(y,random_idx,axis=0)
index=0
N_QUERIES=int(len(X)*40/100)
final_accuracy=0
while index<(N_QUERIES):
    # Randomly picks an unlabelled point from the dataset
    stream_idx = np.random.choice(range(len(X_stream)))
    # Decides whether to query the selected example or not based on a criteria
    if classifier_uncertainty(learner, X[stream_idx].reshape(1, -1)) >= 0.4:
      # If the model decides that the point is useful then it adds the sample to the labelled set and trains the model on the augmented dataset
        learner.teach(X_stream[stream_idx].reshape(1, -1), y_stream[stream_idx].reshape(-1, ))
        new_score = learner.score(X, y)
        # performance_history.append(new_score)
        X_stream, y_stream = np.delete(X_stream, stream_idx, axis=0), np.delete(y_stream, stream_idx)
        print('Index no. %d queried, new accuracy after query no. %d is: %f' % (stream_idx,index+1, new_score))
        index=index+1
        final_accuracy=new_score
print('Final accuracy of the stream based learning object after adding 40% of labelled points using least confidence measure is:',final_accuracy)

# In this part we randomly label points and compare its results with QBC,pool based and stream based scenarios
from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
xm=pd.DataFrame(data=X)
ym=pd.DataFrame(data=y,columns=['digit'])
train=pd.concat([xm,ym], axis=1, sort=False)
# Randomly selects 10% points at random for labelling
train_rand= train.sample(frac=0.1, replace=False, random_state=1)
X_rand=train_rand.drop(columns=['digit'])
y_rand=train_rand['digit']
# Here we use the same classifier(RandomForestClassifier) as the one used in QBC and pool based learning
classifier=RandomForestClassifier()
classifier.fit(X_rand,y_rand);
y_pred=classifier.predict(X_test);
# Calculating accuracy of the model
y_pred=pd.DataFrame(y_pred==y_test)
print("Accuracy of model with randomly 10% selected points for training is :",y_pred[y_pred[0]==True].size/y_pred.size)

# In this part we randomly label points and compare its results with QBC,pool based and stream based scenarios
from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
xm=pd.DataFrame(data=X)
ym=pd.DataFrame(data=y,columns=['digit'])
train=pd.concat([xm,ym], axis=1, sort=False)
# Randomly selects 20% points at random for labelling
train_rand= train.sample(frac=0.2, replace=False, random_state=1)
X_rand=train_rand.drop(columns=['digit'])
y_rand=train_rand['digit']
# Here we use the same classifier(RandomForestClassifier) as the one used in QBC and pool based learning
classifier=RandomForestClassifier()
classifier.fit(X_rand,y_rand);
y_pred=classifier.predict(X_test);
# Calculating accuracy of the model
y_pred=pd.DataFrame(y_pred==y_test)
print("Accuracy of model with randomly 20% selected points for training is :",y_pred[y_pred[0]==True].size/y_pred.size)

# In this part we randomly label points and compare its results with QBC,pool based and stream based scenarios
from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
xm=pd.DataFrame(data=X)
ym=pd.DataFrame(data=y,columns=['digit'])
train=pd.concat([xm,ym], axis=1, sort=False)
# Randomly selects 30% points at random for labelling
train_rand= train.sample(frac=0.3, replace=False, random_state=1)
X_rand=train_rand.drop(columns=['digit'])
y_rand=train_rand['digit']
# Here we use the same classifier(RandomForestClassifier) as the one used in QBC and pool based learning
classifier=RandomForestClassifier()
classifier.fit(X_rand,y_rand);
y_pred=classifier.predict(X_test);
# Calculating accuracy of the model
y_pred=pd.DataFrame(y_pred==y_test)
print("Accuracy of model with randomly 30% selected points for training is :",y_pred[y_pred[0]==True].size/y_pred.size)

# In this part we randomly label points and compare its results with QBC,pool based and stream based scenarios
from modAL.uncertainty import classifier_uncertainty,classifier_entropy,classifier_margin
X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
xm=pd.DataFrame(data=X)
ym=pd.DataFrame(data=y,columns=['digit'])
train=pd.concat([xm,ym], axis=1, sort=False)
# Randomly selects 40% points at random for labelling
train_rand= train.sample(frac=0.4, replace=False, random_state=1)
X_rand=train_rand.drop(columns=['digit'])
y_rand=train_rand['digit']
# Here we use the same classifier(RandomForestClassifier) as the one used in QBC and pool based learning
classifier=RandomForestClassifier()
classifier.fit(X_rand,y_rand);
y_pred=classifier.predict(X_test);
# Calculating accuracy of the model
y_pred=pd.DataFrame(y_pred==y_test)
print("Accuracy of model with randomly 40% selected points for training is :",y_pred[y_pred[0]==True].size/y_pred.size)

# Query by committee part starts here
from modAL.models import ActiveLearner, Committee

X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
n_members = 5
learner_list = list()
from modAL.disagreement import max_disagreement_sampling,vote_entropy_sampling
for member_idx in range(n_members):
    # initial training data
    n_initial = int(len(X)/10)
    train_idx = np.random.choice(range(X.shape[0]), size=n_initial, replace=False)
    X_train = X[train_idx]
    y_train = y[train_idx]

    # creating a reduced copy of the data with the known instances removed
    X = np.delete(X, train_idx, axis=0)
    y = np.delete(y, train_idx)

    # initializing learner
    learner = ActiveLearner(
        estimator=RandomForestClassifier(),
        X_training=X_train, y_training=y_train
    )
    learner_list.append(learner)

# assembling the committee
committee = Committee(learner_list=learner_list,query_strategy=vote_entropy_sampling)

unqueried_score = committee.score(X_test, y_test)
print("The initial accuracy of the committee of 5 RandomForestClassifiers is:",unqueried_score)

performance_history = [unqueried_score]
final_accuracy=0
# query by committee
n_queries = int(len(X)*10/100)

for idx in range(n_queries):
    query_idx, query_instance = committee.query(X)
    X_query, y_query = X[query_index].reshape(1, -1), y[query_index].reshape(1, )
    committee.teach(
        X=X_query,
        y=y_query
    )
    model_accuracy=committee.score(X_test,y_test)
    print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_idx,n=idx + 1, acc=model_accuracy))
    final_accuracy=model_accuracy
    # performance_history.append(model_accuracy)
    # remove queried instance from pool
    X = np.delete(X, query_idx, axis=0)
    y = np.delete(y, query_idx)
print("The final accuracy of the committee after labelling additional 10% points using vote entropy is:",final_accuracy)

from modAL.disagreement import vote_entropy
import pandas as pd
X, y = load_digits(return_X_y=True)
arr=vote_entropy(committee,X)
arr=pd.DataFrame(arr)
ver_space = arr[arr[0] >0]
print("Number of points in version space after adding 10% labelled data points and using vote entropy are:",ver_space.size)
# Greater vote entropy signifies greater disagreement and hence would reduce version space by most
ver_space.sort_values(by=[0],ascending=False)

from modAL.models import ActiveLearner, Committee

X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
n_members = 5
learner_list = list()
from modAL.disagreement import max_disagreement_sampling,vote_entropy_sampling
for member_idx in range(n_members):
    # initial training data
    n_initial = int(len(X)/10)
    train_idx = np.random.choice(range(X.shape[0]), size=n_initial, replace=False)
    X_train = X[train_idx]
    y_train = y[train_idx]

    # creating a reduced copy of the data with the known instances removed
    X = np.delete(X, train_idx, axis=0)
    y = np.delete(y, train_idx)

    # initializing learner
    learner = ActiveLearner(
        estimator=RandomForestClassifier(),
        X_training=X_train, y_training=y_train
    )
    learner_list.append(learner)

# assembling the committee
committee = Committee(learner_list=learner_list,query_strategy=vote_entropy_sampling)

unqueried_score = committee.score(X_test, y_test)
print("The initial accuracy of the committee of 5 RandomForestClassifiers is:",unqueried_score)

performance_history = [unqueried_score]
final_accuracy=0
# query by committee
n_queries = int(len(X)*20/100)
for idx in range(n_queries):
    query_idx, query_instance = committee.query(X)
    X_query, y_query = X[query_index].reshape(1, -1), y[query_index].reshape(1, )
    committee.teach(
        X=X_query,
        y=y_query
    )
    model_accuracy=committee.score(X_test,y_test)
    print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_idx,n=idx + 1, acc=model_accuracy))
    final_accuracy=model_accuracy
    # performance_history.append(model_accuracy)
    # remove queried instance from pool
    X = np.delete(X, query_idx, axis=0)
    y = np.delete(y, query_idx)
print("The final accuracy of the committee after labelling additional 20% points using vote entropy is:",final_accuracy)

from modAL.disagreement import vote_entropy
import pandas as pd
X, y = load_digits(return_X_y=True)
arr=vote_entropy(committee,X)
arr=pd.DataFrame(arr)
ver_space = arr[arr[0] >0]
print("Number of points in version space after adding 20% labelled data points and using vote entropy are:",ver_space.size)
# Greater vote entropy signifies greater disagreement and hence would reduce version space by most
ver_space.sort_values(by=[0],ascending=False)

from modAL.models import ActiveLearner, Committee

X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
n_members = 5
learner_list = list()
from modAL.disagreement import max_disagreement_sampling,vote_entropy_sampling
for member_idx in range(n_members):
    # initial training data
    n_initial = int(len(X)/10)
    train_idx = np.random.choice(range(X.shape[0]), size=n_initial, replace=False)
    X_train = X[train_idx]
    y_train = y[train_idx]

    # creating a reduced copy of the data with the known instances removed
    X = np.delete(X, train_idx, axis=0)
    y = np.delete(y, train_idx)

    # initializing learner
    learner = ActiveLearner(
        estimator=RandomForestClassifier(),
        X_training=X_train, y_training=y_train
    )
    learner_list.append(learner)

# assembling the committee
committee = Committee(learner_list=learner_list,query_strategy=vote_entropy_sampling)

unqueried_score = committee.score(X_test, y_test)
print("The initial accuracy of the committee of 5 RandomForestClassifiers is:",unqueried_score)

performance_history = [unqueried_score]
final_accuracy=0
# query by committee
n_queries = int(len(X)*30/100)
for idx in range(n_queries):
    query_idx, query_instance = committee.query(X)
    X_query, y_query = X[query_index].reshape(1, -1), y[query_index].reshape(1, )
    committee.teach(
        X=X_query,
        y=y_query
    )
    model_accuracy=committee.score(X_test,y_test)
    print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_idx,n=idx + 1, acc=model_accuracy))
    final_accuracy=model_accuracy
    # performance_history.append(model_accuracy)
    # remove queried instance from pool
    X = np.delete(X, query_idx, axis=0)
    y = np.delete(y, query_idx)
print("The final accuracy of the committee after labelling additional 30% points using vote entropy is:",final_accuracy)

from modAL.disagreement import vote_entropy
import pandas as pd
X, y = load_digits(return_X_y=True)
arr=vote_entropy(committee,X)
arr=pd.DataFrame(arr)
ver_space = arr[arr[0] >0]
print("Number of points in version space after adding 30% labelled data points and using vote entropy are:",ver_space.size)
# Greater vote entropy signifies greater disagreement and hence would reduce version space by most
ver_space.sort_values(by=[0],ascending=False)

from modAL.models import ActiveLearner, Committee

X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
n_members = 5
learner_list = list()
from modAL.disagreement import max_disagreement_sampling,vote_entropy_sampling
for member_idx in range(n_members):
    # initial training data
    n_initial =int(len(X)/10)
    train_idx = np.random.choice(range(X.shape[0]), size=n_initial, replace=False)
    X_train = X[train_idx]
    y_train = y[train_idx]

    # creating a reduced copy of the data with the known instances removed
    X = np.delete(X, train_idx, axis=0)
    y = np.delete(y, train_idx)

    # initializing learner
    learner = ActiveLearner(
        estimator=RandomForestClassifier(),
        X_training=X_train, y_training=y_train
    )
    learner_list.append(learner)

# assembling the committee
committee = Committee(learner_list=learner_list,query_strategy=vote_entropy_sampling)

unqueried_score = committee.score(X_test, y_test)
print("The initial accuracy of the committee of 5 RandomForestClassifiers is:",unqueried_score)

performance_history = [unqueried_score]
final_accuracy=0
# query by committee
n_queries = int(len(X)*40/100)
for idx in range(n_queries):
    query_idx, query_instance = committee.query(X)
    X_query, y_query = X[query_index].reshape(1, -1), y[query_index].reshape(1, )
    committee.teach(
        X=X_query,
        y=y_query
    )
    model_accuracy=committee.score(X_test,y_test)
    print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_idx,n=idx + 1, acc=model_accuracy))
    final_accuracy=model_accuracy
    # performance_history.append(model_accuracy)
    # remove queried instance from pool
    X = np.delete(X, query_idx, axis=0)
    y = np.delete(y, query_idx)
print("The final accuracy of the committee after labelling additional 40% points using vote entropy is:",final_accuracy)

from modAL.disagreement import vote_entropy
import pandas as pd
X, y = load_digits(return_X_y=True)
arr=vote_entropy(committee,X)
arr=pd.DataFrame(arr)
ver_space = arr[arr[0] >0]
print("Number of points in version space after adding 40% labelled data points and using vote entropy are:",ver_space.size)
# Greater vote entropy signifies greater disagreement and hence would reduce version space by most
ver_space.sort_values(by=[0],ascending=False)

from modAL.models import ActiveLearner, Committee

X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
n_members = 5
learner_list = list()
from modAL.disagreement import max_disagreement_sampling,vote_entropy_sampling
for member_idx in range(n_members):
    # initial training data
    n_initial = int(len(X)/10)
    train_idx = np.random.choice(range(X.shape[0]), size=n_initial, replace=False)
    X_train = X[train_idx]
    y_train = y[train_idx]

    # creating a reduced copy of the data with the known instances removed
    X = np.delete(X, train_idx, axis=0)
    y = np.delete(y, train_idx)

    # initializing learner
    learner = ActiveLearner(
        estimator=RandomForestClassifier(),
        X_training=X_train, y_training=y_train
    )
    learner_list.append(learner)

# assembling the committee
committee = Committee(learner_list=learner_list,query_strategy=max_disagreement_sampling)

unqueried_score = committee.score(X_test, y_test)
print("The initial accuracy of the committee of 5 RandomForestClassifiers is:",unqueried_score)

performance_history = [unqueried_score]
final_accuracy=0
# query by committee
n_queries = int(len(X)*10/100)
for idx in range(n_queries):
    query_idx, query_instance = committee.query(X)
    X_query, y_query = X[query_index].reshape(1, -1), y[query_index].reshape(1, )
    committee.teach(
        X=X_query,
        y=y_query
    )
    model_accuracy=committee.score(X_test,y_test)
    print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_idx,n=idx + 1, acc=model_accuracy))
    final_accuracy=model_accuracy
    # performance_history.append(model_accuracy)
    # remove queried instance from pool
    X = np.delete(X, query_idx, axis=0)
    y = np.delete(y, query_idx)
print("The final accuracy of the committee after labelling additional 10% points using KL Divergence is:",final_accuracy)

from modAL.disagreement import vote_entropy
import pandas as pd
X, y = load_digits(return_X_y=True)
arr=vote_entropy(committee,X)
arr=pd.DataFrame(arr)
# Vote entropy of zero means all classifiers are in agreement.Therefore,all points with vote entropy greater than zero lie in version space 
# as atleast one classifier disagrees with others wrt that point
ver_space = arr[arr[0] >0]
print("Number of points in version space after adding 10% labelled data points and using KL Divergence are:",ver_space.size)
# Greater vote entropy signifies greater disagreement and hence would reduce version space by most
ver_space.sort_values(by=[0],ascending=False)

from modAL.models import ActiveLearner, Committee

X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
n_members = 5
learner_list = list()
from modAL.disagreement import max_disagreement_sampling,vote_entropy_sampling
for member_idx in range(n_members):
    # initial training data
    n_initial = int(len(X)/10)
    train_idx = np.random.choice(range(X.shape[0]), size=n_initial, replace=False)
    X_train = X[train_idx]
    y_train = y[train_idx]

    # creating a reduced copy of the data with the known instances removed
    X = np.delete(X, train_idx, axis=0)
    y = np.delete(y, train_idx)

    # initializing learner
    learner = ActiveLearner(
        estimator=RandomForestClassifier(),
        X_training=X_train, y_training=y_train
    )
    learner_list.append(learner)

# assembling the committee
committee = Committee(learner_list=learner_list,query_strategy=max_disagreement_sampling)

unqueried_score = committee.score(X_test, y_test)
print("The initial accuracy of the committee of 5 RandomForestClassifiers is:",unqueried_score)

performance_history = [unqueried_score]
final_accuracy=0
# query by committee
n_queries = int(len(X)*20/100)
for idx in range(n_queries):
    query_idx, query_instance = committee.query(X)
    X_query, y_query = X[query_index].reshape(1, -1), y[query_index].reshape(1, )
    committee.teach(
        X=X_query,
        y=y_query
    )
    model_accuracy=committee.score(X_test,y_test)
    print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_idx,n=idx + 1, acc=model_accuracy))
    final_accuracy=model_accuracy
    # performance_history.append(model_accuracy)
    # remove queried instance from pool
    X = np.delete(X, query_idx, axis=0)
    y = np.delete(y, query_idx)
print("The final accuracy of the committee after labelling additional 20% points using KL Divergence is:",final_accuracy)

from modAL.disagreement import vote_entropy
import pandas as pd
X, y = load_digits(return_X_y=True)
arr=vote_entropy(committee,X)
arr=pd.DataFrame(arr)
ver_space = arr[arr[0] >0]
print("Number of points in version space after adding 20% labelled data points and using KL Divergence are:",ver_space.size)
# Greater vote entropy signifies greater disagreement and hence would reduce version space by most
ver_space.sort_values(by=[0],ascending=False)

from modAL.models import ActiveLearner, Committee

X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
n_members = 5
learner_list = list()
from modAL.disagreement import max_disagreement_sampling,vote_entropy_sampling
for member_idx in range(n_members):
    # initial training data
    n_initial = int(len(X)/10)
    train_idx = np.random.choice(range(X.shape[0]), size=n_initial, replace=False)
    X_train = X[train_idx]
    y_train = y[train_idx]

    # creating a reduced copy of the data with the known instances removed
    X = np.delete(X, train_idx, axis=0)
    y = np.delete(y, train_idx)

    # initializing learner
    learner = ActiveLearner(
        estimator=RandomForestClassifier(),
        X_training=X_train, y_training=y_train
    )
    learner_list.append(learner)

# assembling the committee
committee = Committee(learner_list=learner_list,query_strategy=max_disagreement_sampling)

unqueried_score = committee.score(X_test, y_test)
print("The initial accuracy of the committee of 5 RandomForestClassifiers is:",unqueried_score)

performance_history = [unqueried_score]
final_accuracy=0
# query by committee
n_queries = int(len(X)*30/100)
for idx in range(n_queries):
    query_idx, query_instance = committee.query(X)
    X_query, y_query = X[query_index].reshape(1, -1), y[query_index].reshape(1, )
    committee.teach(
        X=X_query,
        y=y_query
    )
    model_accuracy=committee.score(X_test,y_test)
    print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_idx,n=idx + 1, acc=model_accuracy))
    final_accuracy=model_accuracy
    # performance_history.append(model_accuracy)
    # remove queried instance from pool
    X = np.delete(X, query_idx, axis=0)
    y = np.delete(y, query_idx)
print("The final accuracy of the committee after labelling additional 30% points using KL Divergence is:",final_accuracy)

from modAL.disagreement import vote_entropy
import pandas as pd
X, y = load_digits(return_X_y=True)
arr=vote_entropy(committee,X)
arr=pd.DataFrame(arr)
ver_space = arr[arr[0] >0]
print("Number of points in version space after adding 30% labelled data points and using KL Divergence are:",ver_space.size)
# Greater vote entropy signifies greater disagreement and hence would reduce version space by most
ver_space.sort_values(by=[0],ascending=False)

from modAL.models import ActiveLearner, Committee

X, y = load_digits(return_X_y=True)
X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
n_members = 5
learner_list = list()
from modAL.disagreement import max_disagreement_sampling,vote_entropy_sampling
for member_idx in range(n_members):
    # initial training data
    n_initial = int(len(X)/10)
    train_idx = np.random.choice(range(X.shape[0]), size=n_initial, replace=False)
    X_train = X[train_idx]
    y_train = y[train_idx]

    # creating a reduced copy of the data with the known instances removed
    X = np.delete(X, train_idx, axis=0)
    y = np.delete(y, train_idx)

    # initializing learner
    learner = ActiveLearner(
        estimator=RandomForestClassifier(),
        X_training=X_train, y_training=y_train
    )
    learner_list.append(learner)

# assembling the committee
committee = Committee(learner_list=learner_list,query_strategy=max_disagreement_sampling)

unqueried_score = committee.score(X_test, y_test)
print("The initial accuracy of the committee of 5 RandomForestClassifiers is:",unqueried_score)

performance_history = [unqueried_score]
final_accuracy=0
# query by committee
n_queries = int(len(X)*40/100)
for idx in range(n_queries):
    query_idx, query_instance = committee.query(X)
    X_query, y_query = X[query_index].reshape(1, -1), y[query_index].reshape(1, )
    committee.teach(
        X=X_query,
        y=y_query
    )
    model_accuracy=committee.score(X_test,y_test)
    print('Index selected is {m}.Accuracy after query {n}: {acc:0.4f}'.format(m=query_idx,n=idx + 1, acc=model_accuracy))
    final_accuracy=model_accuracy
    # performance_history.append(model_accuracy)
    # remove queried instance from pool
    X = np.delete(X, query_idx, axis=0)
    y = np.delete(y, query_idx)
print("The final accuracy of the committee after labelling additional 40% points using KL Divergence is:",final_accuracy)

from modAL.disagreement import vote_entropy
import pandas as pd
X, y = load_digits(return_X_y=True)
arr=vote_entropy(committee,X)
arr=pd.DataFrame(arr)
ver_space = arr[arr[0] >0]
print("Number of points in version space after adding 40% labelled data points and using KL Divergence are:",ver_space.size)
# Greater vote entropy signifies greater disagreement and hence would reduce version space by most
ver_space.sort_values(by=[0],ascending=False)

from sklearn.cluster import KMeans
import numpy as np
import pandas as pd
X, y = load_digits(return_X_y=True)
X_train=pd.DataFrame(X)
y_train=pd.DataFrame(y)
# Randomly choosing 90% points as the unlabelled dataset
X_train=X_train.sample(random_state=1,frac=0.9,replace=False)
y_train=y_train.sample(random_state=1,frac=0.9,replace=False)

""" Randomly picking 40% points out of the 90% unlabelled set 
for passing as input to clustering algorithm """
X_kmeans=X_train.sample(random_state=1,frac=0.4,replace=False)
y_kmeans=y_train.sample(random_state=1,frac=0.4,replace=False)

# Applying K-Means Algorithm with no. if classes=10
kmeans = KMeans(n_clusters=10, init='k-means++', max_iter=300, n_init=10, random_state=0)
predicted_clusters=kmeans.fit_predict(X_kmeans, y=None, sample_weight=None)

# Preparing the results obtained for further use
cluster_map = pd.DataFrame()
cluster_map['data_index'] = y_kmeans.index.values
cluster_map['cluster'] = predicted_clusters
cluster_map['original']=y_kmeans[0].to_numpy()
cluster_map=cluster_map.sort_values(by=['cluster'])
cluster_map1=cluster_map.copy()

# Getting list of all points assigned 1st cluster by K means 
cluster0=cluster_map1[cluster_map1['cluster']==0]
# Randomly labelling 20% points of the cluster
cluster0=cluster0.sample(random_state=1,frac=0.2,replace=False)
# Getting the majority label out of the 20% points marked
label=cluster0['original'].value_counts().idxmax()
# Using majority label for labelling the reamining points of the cluster
cluster_map.loc[cluster_map.cluster == 0, 'cluster'] = label

# Repeating the above steps for the remaining 9 clusters
cluster1=cluster_map1[cluster_map1['cluster']==1]
cluster1=cluster1.sample(random_state=0,frac=0.2,replace=False)
label=cluster1['original'].value_counts().idxmax()
cluster_map.loc[cluster_map1['cluster'] == 1, "cluster"] = label

cluster2=cluster_map1[cluster_map1['cluster']==2]
cluster2=cluster2.sample(random_state=1,frac=0.2,replace=False)
label=cluster2['original'].value_counts().idxmax()
cluster_map.loc[cluster_map1['cluster'] == 2, "cluster"] = label

cluster3=cluster_map1[cluster_map1['cluster']==3]
cluster3=cluster3.sample(random_state=0,frac=0.2,replace=False)
label=cluster3['original'].value_counts().idxmax()
cluster_map.loc[cluster_map1['cluster'] == 3, "cluster"] = label

cluster4=cluster_map1[cluster_map1['cluster']==4]
cluster4=cluster4.sample(random_state=0,frac=0.2,replace=False)
label=cluster4['original'].value_counts().idxmax()
cluster_map.loc[cluster_map1['cluster'] == 4, "cluster"] = label

cluster5=cluster_map1[cluster_map1['cluster']==5]
cluster5=cluster5.sample(random_state=0,frac=0.2,replace=False)
label=cluster5['original'].value_counts().idxmax()
cluster_map.loc[cluster_map1['cluster'] == 5, "cluster"] = label

cluster6=cluster_map1[cluster_map1['cluster']==6]
cluster6=cluster6.sample(random_state=0,frac=0.2,replace=False)
label=cluster6['original'].value_counts().idxmax()
cluster_map.loc[cluster_map1['cluster'] == 6, "cluster"] = label
# cluster6['original'].value_counts()

cluster7=cluster_map1[cluster_map1['cluster']==7]
cluster7=cluster7.sample(random_state=0,frac=0.2,replace=False)
label=cluster7['original'].value_counts().idxmax()
cluster_map.loc[cluster_map1['cluster'] == 7, "cluster"] = label

cluster8=cluster_map1[cluster_map1['cluster']==8]
cluster8=cluster8.sample(random_state=8,frac=0.2,replace=False)
label=cluster8['original'].value_counts().idxmax()
cluster_map.loc[cluster_map1['cluster'] == 8, "cluster"] = label

cluster9=cluster_map1[cluster_map1['cluster']==9]
cluster9=cluster9.sample(random_state=0,frac=0.2,replace=False)
label=cluster9['original'].value_counts().idxmax()
cluster_map.loc[cluster_map1['cluster'] == 9, "cluster"] = label

# Calculating accuracy of cluster-based labelling
accur=pd.DataFrame(cluster_map['cluster']==cluster_map['original'])
print("Accuracy of clustering algorithm is:",accur[accur[0]==True].size/accur.size)
